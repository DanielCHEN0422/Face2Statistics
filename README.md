# Face2Statistics
A comprehensive roadmap to deliver user-friendly, low-cost and effective alternatives for extracting driversâ€™  statistics. Full paper is accepted by HCII'22.

The WIP version is [here](https://github.com/unnc-ucc/Face2Multimodal).

Daniel Chen: F2S is a roadmap which uses only facial expressions as input to predict multi-modal statistics. When people collect multiple data from drivers to explore better human-vehicle interactions, multiple sensors will inevitably interfere with driving. This motivates us to present F2S. Firstly, it pre-processes images using HSV coding to to mitigate the effects of illuminance. Then we use Deep Neural Network to train models and finally we got the predicted statistics. The strength of F2S is that it is user-friendly, low-cost and effective. And we should develop it to adapt various driving scenes and improve expression recognition accuracy.
=======
[Fig.1](Fig1.png "Fig.1")


